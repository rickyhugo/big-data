---
title: "Homework 3"
output: html_notebook
---

## Task 1

```{r}
# generate data
set.seed(20211106)
N <- 20
x1 <- runif(N, 1, 2)
x2 <- runif(N, 1, 2)
X <- cbind(x1, x2)
y <- ifelse(x2 > -.6 * x1 + 2.35, -1, 1)

# visualize data
plot(X, col = y + 3, pch=15)
```

```{r}
perceptron = function(x, y) {
  converged = F
  n_epochs = 0
  
  iter = dim(x)[1]
  n = dim(x)[2]
  x = as.matrix(x)
  
  # initialize weights
  w = matrix(0, 3, 1)
  while (converged == F) {
    
    # iterate over data set
    for (i in 1:iter) {
      y_pred = as.numeric(sign(t(w) %*% x[i, ]))
      w = w + (y[i] - y_pred) * x[i, ]
    }
    
    # stop training if all observations are 
    # correctly classified
    if ( sum(y * (t(w) %*% t(x)) <= 0) == 0 ) {
      print('Converged sucessfully')
      converged = T
    } else {
      n_epochs = n_epochs + 1
    }
  }
  return(w)
}
```

```{r}
# add bias term to training data
x = cbind(x0 = rep(1, N), X)

# train model
w = perceptron(x, y)
```

```{r}
# compute intercept and slope for decision boundary
intercept = w[1] / -w[3]
slope = w[2] / -w[3]
```


```{r}
# visualize data and decision boundary
png(
  filename = "plots/perceptron.png",
  height = 400,
  width = 600
)
plot(X, col = y + 3, pch = 15)
grid()
abline(a=intercept, b=slope)
```


## Task 2

```{r}
# generate data
rm(list = ls())

#--- generate the data ---#
DGP_ellipse <- function(N = 50, seed = 8312){
  set.seed(seed)
  oval_fun <- function(x,a=1,b=0.5){b*sqrt(1-(x/a)^2)}
  x11 = runif(N, -1, 1)
  x12 = c(oval_fun(x11[1:(.5*N)]),-oval_fun(x11[(.5*N+1):N])) + rnorm(N, 0, 0.05)
  X = cbind(x11, x12)
  x21 = runif(N, -1.5, 1.5)
  x22 = c(oval_fun(x21[1:(.5*N)],a=1.5,b=0.75),-oval_fun(x21[(.5*N+1):N],a=1.5,b=0.75)) + rnorm(N, 0, 0.05)
  X = rbind(X, cbind(x21,x22))
  Q = eigen(matrix(c(1,-4,-4,1),2,2))$vectors
  X = X%*%Q
  y = c(rep(1,N), rep(0, N))
  d = cbind(y, X)
  return(d)
}

N = 10
d = DGP_ellipse(N)
y = d[,1]
X = d[,-1]

# visualize
plot(X, pch=20, col = y+2, xlab = "X1", ylab = "X2", asp = 1, cex = 2)
grid()
#--- generate the data OVER ---#

#--- tr_te_split ---#
id = sample(1:(2*N), N*0.2)
X_tr = X[-id, ]
X_te = X[id, ]
y_tr = y[-id]
y_te = y[id]
#--- tr_te_split OVER ---#
```

```{r}
library(Jmisc)
```

```{r}
get_gram = function(x) {
  n = dim(x)[1]
  res = matrix(0, n, n)
  for (i in 1:n) {
    for (j in 1:n) {
      res[i, j] = sum(x[i, ] * x[j, ])^2
    }
  }
  return(res)
}
```


```{r}
kernel_func = function(x) {
  x1 = x[ , 1]
  x2 = x[ , 2]
  res = cbind(x1^2, sqrt(2) * x1 * x2, x2^2)
  return(res)
}
```


```{r}
# ordinary PCA on augmented data for validation
n = dim(X_tr)[1]
C = matrix(1 / n, n, n)
x_trans = kernel_func(X_tr)
x_d = demean(x_trans)
S = t(x_d) %*% x_d

e_vec = eigen(S)$vectors
pc = x_d %*% e_vec
plot(
  pc[, 3], rep(0, n), col = y_tr + 2, 
  ylab='', xlab='Z3', cex=2, pch=20, main="PCA (validation)"
)
grid()
```

```{r}
# compute gram matrix
K = get_gram(X_tr)

# centralize
n = dim(X_tr)[1]
C = matrix(1 / n, n, n)
identity = diag(1, n, n)
K_star = (identity - C) %*% K %*% (identity - C)
```

```{r}
# eigen decomposition of the gram matrix
eigen_space = eigen(K_star)
e_values = eigen_space$values
e_vectors = eigen_space$vectors
```


```{r}
# visualize
plot(
  e_vectors[ , 3], rep(0, n), pch = 20, col = y_tr + 2,
  ylab = "", xlab = "Z3", cex = 2, main = "KPCA"
)
grid()
```

```{r}
# kernelize test set
K_vec = matrix(0, n, 2)

for (i in 1:n) {
  K_vec[i, 1] = sum(X_tr[i, ] * X_te[1, ])^2
  K_vec[i, 2] = sum(X_tr[i, ] * X_te[2, ])^2
}
```

```{r}
# predict test set
one_n = matrix(1 / n, n, 1)
scaled_u = e_vectors[, 3] / e_values[3]
d = dim(K_vec)[2]
test_obs = matrix(0, 2, 1)
for (i in 1:d) {
  test_obs[i, 1] = scaled_u %*% (identity - C) %*% (K_vec[ , i] - K %*% one_n) 
}
```

```{r}
plot(
  e_vectors[ , 3], rep(0, n), pch = 20, col = y_tr + 2,
  ylab = "", xlab = "Z3", cex = 2, main = "KPCA with predictions"
)
grid()
points(test_obs[1], 0.2, pch=4, cex=1.5, col=y_te[1] + 2)
points(test_obs[2], 0.2, pch=4, cex=1.5, col=y_te[2] + 2)
legend(
  "topright", inset = .02,
  legend=c('Test 1', 'Test 2'),
  col=2:3, pch=4
)
```

## Task 3

```{r}
rm(list = ls())
```


```{r}
library(kernlab)
library(caret)
```


```{r}
colSds <- function(x, na.rm=TRUE) {
  if (na.rm) {
    n <- colSums(!is.na(x))
  } else {
    n <- nrow(x)
  }
  colVar <- colMeans(x*x, na.rm=na.rm) - (colMeans(x, na.rm=na.rm))^2
  return(sqrt(colVar * n / (n - 1)))
}
```


```{r}
get_gaussian_gram = function(x, sigma) {
  set.seed(2020)

  # define kernel
  kernel = rbfdot(sigma = sigma)
  
  # compute gram matrix
  K = kernelMatrix(kernel = kernel, x = x)
  
  n = dim(x)[1]
  d = K@.Data
  return(d)
}
```

```{r}
get_prediction_gram = function(train, val, sigma) {
  set.seed(2020)
  
  # define kernel
  kernel = rbfdot(sigma = sigma)
  
  # compute gram matrix for prediction
  n = dim(train)[1]
  d = dim(val)[1]
  res = matrix(0, n, d)
  for (i in 1:n) {
    for (j in 1:d) {
      res[i, j] = kernel(train[i, ], val[j, ])
    }
  }
  return(res)
}
```


```{r}
split_data = function(data, size=400) {
  set.seed(2020)
  
  # split data into test and training sets
  x = data.matrix(data)
  n = dim(x)[1]
  train_id = sample(1:n, size)
  
  n_cols = ncol(data)
  feature_col = 1:(n_cols - 1)
  response_col = n_cols
  
  features = x[ , feature_col] # predictors
  response = x[ , response_col] # response
  
  x_train = features[train_id, ]
  x_train_scaled = scale(x_train)
  y_train = response[train_id]
  
  x_test = features[-train_id, ]
  x_test_scaled = scale(
    x_test,
    center = colMeans(x_train),
    scale = colSds(x_train)
  )
  y_test = response[-train_id]
  
  return(
    list(
      trainX=x_train_scaled,
      trainY=matrix(y_train),
      X=x_test_scaled,
      Y=matrix(y_test)
    )
  )
}
```

```{r}
get_RMSE = function(prediction, label) {
  return(sqrt(mean((label - prediction)^2)))
}
```


```{r}
k_fold_cv = function(k, param_grid, x, y) {
  set.seed(2020)
  
  # split data into k even folds
  folds = createFolds(y, k = 10)
  
  k = length(folds) # no. folds for CV
  n = dim(param_grid)[1] # no. parameter settings
  fold_index = matrix(1:k)
  
  RMSE_mat = matrix(0, k, n)
  
  for (i in 1:k) {
    # extract data for current k-1 folds
    which_folds = fold_index[fold_index != i]
    train_idx = as.numeric(unlist(folds[which_folds]))
    train_data = x[train_idx, ]
    train_labels = y[train_idx, ]
    val_data = x[-train_idx, ]
    val_labels = y[-train_idx, ]
    for (p in 1:n) {
      # fit a model for each parameter setting
      sigma = param_grid$sigma[p]
      lambda = param_grid$lambda[p]
      
      K_train = get_gaussian_gram(train_data, sigma)
      K_val = get_prediction_gram(train_data, val_data, sigma)

      w = get_weights(K_train, train_labels, lambda)
      val_pred = t(w) %*% K_val
      RMSE_mat[i, p] = get_RMSE(val_pred, val_labels) 
    }
  }
  return(colMeans(RMSE_mat, na.rm = T)) # return average RMSE
}
```


```{r}
get_weights = function(K, y, lambda) {
  n = dim(K)[1]
  identity = diag(1, n, n)
  w = solve(K + lambda * identity) %*% y
  return(w)
}
```


```{r}
predictKRR = function(trainX, trainY, X, lambda, sigma) {
  # kernelize data
  K_train = get_gaussian_gram(trainX, sigma)
  K_test = get_prediction_gram(trainX, X, sigma)
  
  # compute weights
  w = get_weights(K_train, trainY, lambda)
  
  # get predictions
  pred = t(w) %*% K_test
  return(pred)
}
```

```{r}
# read data 
data = read.table("data/Boston.txt", header = T, sep = ",")
```

```{r}
# get scaled test and training data
d = split_data(data)

# train
trainX = d$trainX
trainY = d$trainY

# test
X = d$X
Y = d$Y
```

```{r}
set.seed(2020)

# ordinary linear regression
lr_train = as.data.frame(trainX)
lr_train$medv = trainY
lr = lm(medv ~ ., data=lr_train)
```

```{r}
# test lr model
lr_y_hat = predict(lr, as.data.frame(X))
```


```{r}
LR_RMSE = get_RMSE(as.numeric(lr_y_hat), Y)

png(
  filename = "plots/lr_pred.png",
  height = 400,
  width = 600
)
plot(
  lr_y_hat, col = 4, pch = 16,
  main = "LR Predictions",
  sub = paste("RMSE:", round(LR_RMSE, 2)),
  ylab = "medv",
  xlab = "Observation\n"
)
points(Y, col = 1, pch=17) # true values
grid()
legend(
  "topright", inset = .02,
  legend = c("Prediction", "True value"),
  col = c(4, 1), pch = c(16, 17)
)
```

```{r}
set.seed(2020)

# find optimal hyperparameters by 10-fold CV
param_grid = expand.grid(
  lambda = c(.1, .01, .001),
  sigma = c(.1, .01, .001)
)
cv_params = k_fold_cv(k = 10, param_grid, trainX, trainY)
```

```{r}
set.seed(2020)

# extract optimal hyperparameters
param_idx = match(min(cv_params), cv_params)
opti_params = param_grid[param_idx, ]

# fit optimal model and predict test set
y_hat = predictKRR(
  trainX, trainY, X, opti_params$lambda, opti_params$sigma
)
```

```{r}
KRR_RMSE = get_RMSE(t(y_hat), Y)

png(
  filename = "plots/krr_pred.png",
  height = 400,
  width = 600
)
plot(
  y_hat[1, ], col = 3, pch = 16,
  main = "KRR Predictions",
  sub = paste("RMSE:", round(KRR_RMSE, 2)),
  ylab = "medv",
  xlab = "Observation\n"
)
points(Y, col = 1, pch=17) # true values
grid()
legend(
  "topright", inset = .02,
  legend = c("Prediction", "True value"),
  col = c(3, 1), pch = c(16, 17)
)
```

```{r}
predictKRR_ = function(trainX,trainY,X,lambda,sigma){
  
  rbf <- rbfdot(sigma)
  K = kernelMatrix(rbf, trainX)
  K_test = kernelMatrix(rbf, trainX, X)
  I = diag(dim(trainY)[1])
  pred = t(trainY) %*% solve(K + lambda*I) %*% K_test

  return(pred)
}
```
```{r}
y_EBBA = predictKRR(
  trainX, trainY, X, opti_params$lambda, opti_params$sigma
)
get_RMSE(t(y_EBBA), Y)
```


```{r}
library(listdtr)
set.seed(2020)

# built-in KRR for comparison
test_model = krr(trainX, trainY)
```


```{r}
# predict test set and compute RMSE
comparison_y_hat = predict(test_model, X)
comparison_RMSE = get_RMSE(comparison_y_hat, Y)
paste("Built-in KRR RMSE:", round(comparison_RMSE, 2))
```

